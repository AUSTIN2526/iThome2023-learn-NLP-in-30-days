{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f03441",
   "metadata": {},
   "source": [
    "# 斷詞與過濾重複的Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c94ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are', 'human', 'language', 'natural', 'robot', 'like', 'Hello', 'a', 'Apple', 'love', 'Python', 'am', 'You', 'processing', 'I'}\n"
     ]
    }
   ],
   "source": [
    "#假設該資料集中的句子如english_sentence\n",
    "english_sentence = [\n",
    "    'I love natural language processing',\n",
    "    'Hello Python',\n",
    "    'I like Apple',\n",
    "    'I am a human',\n",
    "    'You are a robot',\n",
    "]\n",
    "\n",
    "tokens = []\n",
    "for sentence in english_sentence:\n",
    "    tokens.extend(sentence.split(' '))  # 將一段句字進行斷詞後加入List(列表)\n",
    "tokens = set(tokens)                    # 通過set()過濾重複單字\n",
    "print(tokens)                           # 注意此時的資料型態是集合(Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4922a3d",
   "metadata": {},
   "source": [
    "# 加入特殊識別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c743cc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[PAD]', 'are', 'human', 'language', 'natural', 'robot', 'like', 'Hello', 'a', 'Apple', 'love', 'Python', 'am', 'You', 'processing', 'I']\n"
     ]
    }
   ],
   "source": [
    "special_token = ['[UNK]','[PAD]']       # 建立特殊的詞彙表\n",
    "tokens = special_token + list(tokens)   # Tokens為Set型態，因此需要轉型成List才能夠相加\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bbacf",
   "metadata": {},
   "source": [
    "# 建立Token和數字互相轉換的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f062a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[UNK]': 0, '[PAD]': 1, 'are': 2, 'human': 3, 'language': 4, 'natural': 5, 'robot': 6, 'like': 7, 'Hello': 8, 'a': 9, 'Apple': 10, 'love': 11, 'Python': 12, 'am': 13, 'You': 14, 'processing': 15, 'I': 16}\n"
     ]
    }
   ],
   "source": [
    "token2num = {tokens:num for num, tokens in enumerate(tokens)}\n",
    "print(token2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ede608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[UNK]', 1: '[PAD]', 2: 'are', 3: 'human', 4: 'language', 5: 'natural', 6: 'robot', 7: 'like', 8: 'Hello', 9: 'a', 10: 'Apple', 11: 'love', 12: 'Python', 13: 'am', 14: 'You', 15: 'processing', 16: 'I'}\n"
     ]
    }
   ],
   "source": [
    "num2token = {num:tokens for num, tokens in enumerate(tokens)}\n",
    "print(num2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f638b813",
   "metadata": {},
   "source": [
    "# 建立最終的Tokeinzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b030bd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始輸入: I like Banana\n",
      "轉換結果: [16, 7, 0, 1, 1]\n",
      "還原結果: I like [UNK] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(input_text, token2num, max_len = 5):\n",
    "    UNK_IDX = token2num['[UNK]']                 # 取得未知詞彙的索引值\n",
    "    PAD_IDX = token2num['[PAD]']                 # 取得填充詞彙的索引值\n",
    "    \n",
    "    tokens = input_text.split(' ')               # 斷詞\n",
    "\n",
    "    output_num = []\n",
    "    for token in tokens:\n",
    "        num = token2num.get(token, UNK_IDX)      # 轉換成數字(不存在於字典時轉換成[UNK])\n",
    "        output_num.append(num)\n",
    "        \n",
    "    padding_num = max_len - len(output_num)      # 計算需填充的數量\n",
    "    return output_num + [PAD_IDX] * padding_num  # 補齊最大長度\n",
    "\n",
    "\n",
    "input_text = 'I like Banana'\n",
    "output_num = tokenizer(input_text, token2num)\n",
    "print(f'原始輸入: {input_text}')\n",
    "print(f'轉換結果: {output_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761c13a",
   "metadata": {},
   "source": [
    "# 數字轉文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1604ed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "還原結果: I like [UNK] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "def num2tokens(input_list):\n",
    "    output_list = [num2token[num] for num in input_list]\n",
    "    return ' '.join(output_list)\n",
    "\n",
    "restore_text = num2tokens(output_num)\n",
    "print(f'還原結果: {restore_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa0d503",
   "metadata": {},
   "source": [
    "# 完整程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64affef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始輸入: I like Banana\n",
      "轉換結果: [16, 7, 0, 1, 1]\n",
      "還原結果: I like [UNK] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, english_sentence, max_len = 5, special_token = None, padding = True):\n",
    "        \n",
    "        tokens = []\n",
    "        for sentence in english_sentence:\n",
    "            tokens.extend(sentence.split(' '))  # 將一段句字進行斷詞後加入列表(List)\n",
    "        tokens = set(tokens)                    # 通過set()過濾重複單字\n",
    "        \n",
    "        if special_token is not None:\n",
    "            tokens = special_token + list(tokens)\n",
    "        \n",
    "        self.token2num = {tokens:num for num, tokens in enumerate(tokens)}\n",
    "        self.num2token = {num:tokens for num, tokens in enumerate(tokens)}\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.padding = padding\n",
    "    \n",
    "    def __call__(self, input_text):\n",
    "        tokens = input_text.split(' ')              \n",
    "        UNK_IDX = self.token2num['[UNK]']\n",
    "        PAD_IDX = self.token2num['[PAD]'] \n",
    "\n",
    "        output_num = []\n",
    "        for token in tokens:\n",
    "            num = self.token2num.get(token, UNK_IDX)  # 轉換成數字(不存在於字典時轉換成UNK_IDX)\n",
    "            output_num.append(num)\n",
    "            \n",
    "        padding_num = self.max_len - len(output_num)  # 計算需填充的數量\n",
    "        return output_num + [PAD_IDX] * padding_num   # 補齊最大長度\n",
    "       \n",
    "    \n",
    "    def num2tokens(self, input_list):\n",
    "        output_list = [self.num2token[num] for num in input_list]\n",
    "        return ' '.join(output_list)\n",
    "    \n",
    "    \n",
    "# 所有句子\n",
    "english_sentence = [\n",
    "    'I love natural language processing',\n",
    "    'Hello Python',\n",
    "    'I like Apple',\n",
    "    'I am a human',\n",
    "    'You are a robot',\n",
    "]\n",
    "\n",
    "# 建立初始值\n",
    "tokenizer = Tokenizer(english_sentence, special_token = ['[UNK]','[PAD]'])\n",
    "\n",
    "#使用建立的Tokeizer\n",
    "input_text = 'I like Banana'\n",
    "output_num = tokenizer(input_text)\n",
    "restore_text = tokenizer.num2tokens(output_num)\n",
    "\n",
    "#顯示結果\n",
    "print(f'原始輸入: {input_text}')\n",
    "print(f'轉換結果: {output_num}')\n",
    "print(f'還原結果: {restore_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00be3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
