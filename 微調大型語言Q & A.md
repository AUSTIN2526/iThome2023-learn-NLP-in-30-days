### 1. 在微調LLM時會發生記憶體不足的問題，該怎麼處理?  
* LLM通常是指參數量7B以上的語言模型，而在這麼大的參數量之下微調就需要多張顯卡的記憶分別儲存模型與訓練資料，但不是一種解決方式，最好的解決方式應該是使用**模型量化**來解決。  

### 2. 常見的模型量化方式有4位元或8位元的量化，若量化模型會發生什麼事?
* 若在使用模型時使用4位元或8位元的量化方式這會導致模型的精度丟失，若我們直接將這個精度丟失的模型進行微調將會導致**文字在生成時常常無法判斷出EOS token或造成生成出來的文字錯誤**的狀況。

### 3. 該如何解決4位元或8位元精度丟失的問題?
* 精度丟失是不可避免的，但最嚴重的狀況就是在**精度丟失的狀況下去微調模型**，這就會導致Q1的狀況發生，最好的方式是使用LoRA這類的adapter技術來解決，在原始的**模型區塊中加入一個旁路分支**，這樣子模型就只會更新旁路分支的權重而不影響到原始模型的狀況。

### 4. LoRA不是會加速模型的訓練速度嗎? 怎麼越用越慢?
* 先說結論，LoRA是一種高效的計算方式，不是一種快速的訓練方式。因為反向傳播還需計算增加分支，雖然前向傳播時只需計算分支而不用被adapter的主體，但計算量最多的地方是在反向傳播中，因此計算時間會變得更久。

### 5. 那為何大家都說LoRA的運算速度很快?
* 因為比較的對象是全量微調(也就是在沒量化的狀態)，而LoRA這些技術的期刊通常都提出了兩個貢獻，第一個是提出一種模型量化的方式，第二個則是修正這種量化方式所帶來的問題。因此在LoRA這項技術中其實就是8bit量化 + low rank旁路的結合，**真正加速的地方在於8bit量化技術(與bfloat16這個資料型態)**而非low rank旁路，所以對於全量微調的狀況下LoRA的速度確實能快3~4倍。

### 6 使用LoRA後的模型推理速度會變慢該怎麼解決?
* 使用LoRA後會多出了更多計算量，所以前向傳播的速度會變更慢，而在這部分我們可以將LoRA的權重融合到原始的模型中來解決這個狀況。

### 7. 雖然有些官方文件上寫4位元或8位元的量化不能直接finetune，但到底能不能這樣做?
* 可以，但是會發生Q1的狀況，具體來說就是會造成Recall下降，但邏輯推理上還是差不多的。

### 8. 為何在微調LLM中都會使用STFTrainer而不是Trainer或自己撰寫程式?
* STFTrainer中有內建Hugging face的peft庫，所以我們在使用STFTrainer時只需要給予LoRAConfig他就會自動幫我們轉換成LoRA的模型了，而使用Trainer的話我們必須加入get_peft_model()包裝模型後再給予Trainer，簡單來說就是**因為比較方便**。

### 9. prompt與instruct到底差在哪裡?
* prompt是針對文本補全能力，instruct則是增進模型推理能力，兩者的定義其實差不多，這其實蠻主觀的。唯一有差別的是prompt learning與instruct learning，prompt learning就是讓模型**在一個任務上進行微調**，使模型在該任務上能夠進行識別，而instruct learning則是在**多個任務上進行微調**，以適應其他任務。
